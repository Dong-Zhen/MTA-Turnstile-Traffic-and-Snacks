{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "Help a nonprofit organization YoLocal Snack find a location to open up a store. Their goal is to provide nourishing local snacks and drinks to daily commuters during common meal hours. To make the most impact, YoLocal Snack's ideal store location is by a subway station in a local community with a large population of New Yorkers with long weekday commute times.  \n",
    "\n",
    "#### Common Meal Hours\n",
    "- 8am-12pm = breakfast \n",
    "- 12pm-4pm = lunch\n",
    "- 4pm-8pm = happy hour\n",
    "\n",
    "### Find the High Traffic Commuter Stations\n",
    "\n",
    "We want high entry numbers in the morning and high exit numbers in the evening, and for the morning entry traffic and evening traffic to be similar by the end of the day. This indicates a strong population of 9 to 5 workers and students residing near a particular station.\n",
    "\n",
    "```Note: Within these commuter stations are outliers with very high traffic because New Yorkers travel to certain stations to exit for work or transfer to another form of transportation. All of these stations are in Manhattan and will be excluded from our list of commuter stations because it is difficult to determine if the commuters in Manhattan are traveling a long distance. We can use this data to show where commuters are in certain points of day.```\n",
    "\n",
    "### Establishing Long Distance Commuters\n",
    "\n",
    "The NYC MTA Subway map reveals that the majority of subway stations in boroughs outside of Manhattan contain only one or two lines. These stations serve different neighborhoods and the distance between each line and Manhattan establishes a clear distinction of a long commute.  \n",
    "\n",
    "### Further checks\n",
    "\n",
    "After identifying potential stations, we can do a google search of the station to see how many local snack stores are nearby. Google's activity tracker reveals if the stores see an increase in traffic during meal hours. In the future, YoLocal Snack will work with these vendors to efficiently cater to local commuters. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Data\n",
    "\n",
    "MTA data from January 2021 to April 2021 will form the basis of my analysis. This is a good time frame to look at New York's commuter cycle. Students go back to school in January and workers resume work after major holidays. Additionally, the turnstile data has reset so it's possible to detect where anomalies begin and decide what to do with them. Decreases in commuter traffic due to COVID is not a concern because YoLocal Snack is serving New Yorkers who need to commute to work.\n",
    "\n",
    "\n",
    "### MTA Turnstile Data \n",
    "\n",
    "##### Purpose: \n",
    "\n",
    "- Entry and Exit Traffic Numbers\n",
    "- Stations and Line Names\n",
    "- Date and Time  \n",
    "\n",
    "### MTA Location Data\n",
    "\n",
    "##### Purpose: \n",
    "\n",
    "- Borough\n",
    "- Latitude and Longitude\n",
    "\n",
    "### MTA RIDERSHIP DATA\n",
    "\n",
    "##### Purpose: \n",
    "\n",
    "- The total ridership from January 1, 2021 to April 23, 2021 is [171,715,108](https://new.mta.info/document/20441)\n",
    "- Estimate daily ridership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import calendar\n",
    "import datetime \n",
    "from datetime import timedelta\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max.colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_total_riders_url = \"https://new.mta.info/document/20441\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "\n",
    "Make a daily total dataframe by loading the csv with pandas. \n",
    "\n",
    "Access the mta database with sql alchemy and left join the turnstile table with location data by Stop_Name and Division. Then create a dataframe with the joined tables.\n",
    "\n",
    "- \" WITH turnstile21_data as \n",
    "- (SELECT * FROM turnstile_data WHERE DATE LIKE '%2021')\n",
    "- SELECT t.*, l.Borough, l.GTFS_Latitude, l.GTFS_Longitude\n",
    "- FROM turnstile21_data t LEFT JOIN location_data l\n",
    "- ON t.STATION = upper(l.Stop_Name) AND t.DIVISION = l.Division;\"\n",
    "\n",
    "The top code works, but takes a long time to load on dbrowser so I will not use it on jupyter notebook. We'll have to import them individually and join them through pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_total_raw_df = pd.read_csv(daily_total_riders_url)\n",
    "daily_total_raw_df.rename(columns = {'Date': 'DATE', 'Subways: Total Estimated Ridership':'SUBWAY_TOTAL'}, inplace = True)\n",
    "daily_total_df = daily_total_raw_df.loc[:, ['DATE','SUBWAY_TOTAL']]\n",
    "daily_total_df['YEAR'] = daily_total_df['DATE'].str.extract(r'\\b(\\d+)$')\n",
    "daily_total_21_df = daily_total_df[daily_total_df['YEAR'] == '2021']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"sqlite:///Data/mta.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnstile_df_21 = pd.read_sql(\"SELECT * FROM turnstile_data WHERE DATE LIKE '%2021';\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnstile_df_21.columns = turnstile_df_21.columns.str.replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df = pd.read_sql(\"SELECT Stop_Name, Division, Borough, GTFS_Latitude, GTFS_Longitude FROM location_data;\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df['Stop_Name'] = location_df['Stop_Name'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_loc_21_df = turnstile_df_21.merge(location_df, how = 'left', left_on=['STATION'], right_on=['Stop_Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_loc_21_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_loc_21_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_loc_21_df['ENTRIES'] = turn_loc_21_df['ENTRIES'].astype('int')\n",
    "turn_loc_21_df['EXITS'] = turn_loc_21_df['EXITS'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_loc_21_df['ENTRIES'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_loc_21_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANING Part 1\n",
    "\n",
    "A quick exploration of the dataset reveals many cleaning tasks. There are duplicate rows, the exits and entries columns contain outliers that are far from the mean, the time column reveals 62524 instead of the expected 7. The entries and exits columns show cumulative values instead of the number of entries at that point in time. \n",
    "\n",
    "The next steps will include:\n",
    "1. Reformat the unique 62524 time values to the standard \n",
    "2. Combine DATE and TIME and create an indentification for a unique turnstile\n",
    "3. Create a Day column and weekday mask  \n",
    "4. Locate the outliers greater than a hundred million because there were only 171,715,108 total riderships across stations in this time period.  \n",
    "5. Removing the duplicate values \n",
    "6. Calculate the number of entries and exits for each turnstile  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_time = turn_loc_21_df['TIME'].reset_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_time['first3'] = temp_time['TIME'].str[:4]\n",
    "temp_time['first3'] = temp_time['first3'].str.replace(\":\", \".\").astype('float')\n",
    "temp_time['first3'] = temp_time['first3'].apply(lambda x: np.round(x,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bin = [0.0, 4.0, 8.0, 12.0, 16.0, 20.0, 24.0]\n",
    "temp_time['first3'] = temp_time['first3'].apply(lambda x: time_bin[np.digitize(x,time_bin, right = True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_time['first3'] = temp_time['first3'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dict = {'0.0': '00:00:00' , '4.0': '04:00:00' , '8.0': '08:00:00' , '12.0': '12:00:00', \n",
    "             '16.0': '16:00:00', '20.0': '20:00:00', '24.0': '00:00:00' }\n",
    "temp_time['first3'] = temp_time['first3'].map(time_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_list = list(temp_time['first3'])\n",
    "turn_loc_21_df['TIME'] = time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_loc_21_df['TIME'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_dfs = [turn_loc_21_df]\n",
    "\n",
    "for mta_df in mta_dfs:\n",
    "    \n",
    "    mta_df['DATETIME'] = pd.to_datetime(mta_df.DATE + \" \" + mta_df.TIME, \n",
    "                                        format=\"%m/%d/%Y %H:%M:%S\")\n",
    "    \n",
    "    mta_df['TURNSTILES'] = mta_df['C/A'] + \"-\" +\\\n",
    "                           mta_df['UNIT'] + \"-\" +\\\n",
    "                           mta_df['SCP'] + \"-\" +\\\n",
    "                           mta_df['STATION'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_week_dict = dict(enumerate(calendar.day_name))\n",
    "turn_loc_21_df['DAYNAME'] = turn_loc_21_df['DATETIME'].dt.weekday.map(day_of_week_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_mask = (turn_loc_21_df['DAYNAME'].isin(calendar.day_name[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_loc_21_df = turn_loc_21_df[['TURNSTILES', 'C/A', 'UNIT', 'SCP', 'STATION', 'LINENAME', \n",
    "                                   'BOROUGH', 'GTFS_LATITUDE', 'GTFS_LONGITUDE', 'DATETIME', \n",
    "                                   'DATE', 'DAYNAME', 'TIME','ENTRIES', 'EXITS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(turn_loc_21_df.groupby(['TURNSTILES','DATETIME'])\n",
    "['ENTRIES', 'EXITS'].count()\n",
    ".reset_index()\n",
    ".sort_values([\"ENTRIES\", \"EXITS\"], ascending=False)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_loc_21_df.sort_values(['TURNSTILES','DATETIME'], \n",
    "                   ascending = True, inplace = True)\n",
    "turn_loc_21_df.drop_duplicates(subset = ['TURNSTILES', 'DATETIME'], keep = 'first',\n",
    "                      inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_loc_21_df[[\"PREV_DATE\", \"PREV_ENTRIES\", \"PREV_EXITS\"]] = (turnstile_df_21\n",
    "                                                       .groupby([\"TURNSTILES\"])[\"DATE\", \"ENTRIES\", \"EXITS\"]\n",
    "                                                       .apply(lambda grp: grp.shift(1)))\n",
    "turn_loc_21_df.dropna(subset=[\"PREV_DATE\"], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_traffic_21 = daily_total_21_df.groupby('YEAR')['SUBWAY_TOTAL'].sum()['2021']\n",
    "print(total_traffic_21)\n",
    "print(len(str(total_traffic_21)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_loc_21_df['IRR_ENTRIES']=turn_loc_21_df['ENTRIES'].apply(lambda x: len(str(x))>=8) \n",
    "turn_loc_21_df['IRR_EXITS']=turn_loc_21_df['EXITS'].apply(lambda x: len(str(x))>=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_turnstile_df_21 = turn_loc_21_df[~(turn_loc_21_df['irr_entry'] == True) & \n",
    "                                        ~(turn_loc_21_df['irr_exit'] == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_turnstile_df_21.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANING Part 2\n",
    "\n",
    "We reformatted the dates to fall into date ranges 0-4,4-8,8-12,4-8,8-12 to make it easier for analysis. The outliers that fall outside of the total number of traffic in January to April 2021 have been masked and removed. \n",
    "\n",
    "Before we calculate the entries and exits for a particular point in time, we need to perform a gutcheck. The ideal stiatuion is to have all the previous entries be less than the current entries. We want to check if there are situations where PREV_ENTRIES > ENTRIES or PREV_EXITS > EXITS and then decide how to calculate the entries and exits. We can sort the entries and exits descending order with a groupby. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (turnstile_df_21[\"ENTRIES\"] < turnstile_df_21[\"PREV_ENTRIES\"])\n",
    "turnstile_df_21[mask].groupby([\"TURNSTILES\"]).size().sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_mask = (clean_turnstile_df_21[\"ENTRIES\"] < clean_turnstile_df_21[\"PREV_ENTRIES\"])\n",
    "clean_turnstile_df_21[mask].groupby([\"TURNSTILES\"]).size().sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_counter = 4 * 3600 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entries_diff(row, max_counter):\n",
    "    counter = abs(row['ENTRIES'] - row['PREV_ENTRIES'])\n",
    "    if counter > max_counter:\n",
    "        counter = min(row[\"ENTRIES\"], row[\"PREV_ENTRIES\"])\n",
    "    if counter > max_counter:\n",
    "        return 0\n",
    "    return counter\n",
    "def exits_diff(row, max_counter):\n",
    "    counter = abs(row['EXITS'] - row['PREV_EXITS'])\n",
    "    if counter > max_counter:\n",
    "        counter = min(row[\"EXITS\"], row[\"PREV_EXITS\"])\n",
    "    if counter > max_counter:\n",
    "        return 0\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_turnstile_df_21['ENTRIES_COUNT'] = clean_turnstile_df_21.apply(entries_diff, axis = 1, max_counter = max_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_turnstile_df_21['ENTRIES_COUNT'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_turnstile_df_21['EXITS_COUNT'] = clean_turnstile_df_21.apply(exits_diff, axis = 1, max_counter = max_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_turnstile_df_21['TOTAL_COUNT'] = clean_turnstile_df_21['ENTRIES_COUNT'] + clean_turnstile_df_21['EXITS_COUNT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ANALYSIS WITH ONLY TURNSTILE DATA\n",
    "\n",
    "After finding the entries and exits values, we can combine the two values to find the total traffic for a particular turnstile at a time in day. \n",
    "\n",
    "Questions:\n",
    "\n",
    "1. Find the top 20 stations with the highest number of exits, enteries, traffic\n",
    "    - Now find the top stations with only one or two lines with the highest number of exits, entries, traffic\n",
    "2. Using the results from question one, we find the stations with highest exits, entries, traffics for time ranges 8-12, 12-4, 4-8 *meal hours\n",
    "    \n",
    "    - Which stations have the most entries around 8-12 am\n",
    "    - Which stations have the most exits around 4-8pm pm\n",
    " \n",
    "    \n",
    "    - Which stations have the most exits around 8-12 am?\n",
    "    - Which stations have the most entriess around 4 - 8 pm? \n",
    "    \n",
    "3. Find the average total of exits, entries, traffic for each weekday\n",
    "    - Do entries = exits?\n",
    "    - Is traffic consistent throughout the weekdays\n",
    "    - Using total traffic establish percentage of people in certain stations?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_entries = clean_turnstile_df_21.groupby('STATION')['ENTRIES_COUNT'].sum().head(20).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_turnstile_df_21[weekday_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_exits = clean_turnstile_df_21.groupby('STATION')['EXITS_COUNT'].sum().head(20).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_traffic = clean_turnstile_df_21.groupby('STATION')['TOTAL_COUNT'].sum().head(20).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (clean_turnstile_df_21['LINENAME'].str.len() == 2)\n",
    "two_or_one_line_df = clean_turnstile_df_21[mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_entries_2 = two_or_one_line_df.groupby('STATION')['ENTRIES_COUNT'].sum().head(20).sort_values(ascending = False)\n",
    "top_20_entries_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_exits_2 = two_or_one_line_df.groupby('STATION')['EXITS_COUNT'].sum().head(20).sort_values(ascending = False)\n",
    "top_20_exits_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_traffic_2 = two_or_one_line_df.groupby('STATION')['TOTAL_COUNT'].sum().head(20).sort_values(ascending = False)\n",
    "top_20_traffic_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_total_check_df_21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA VIZUALIZATIONS WITH ONLY TURNSTILE DATA\n",
    "\n",
    "Plot the answers to the questions to itentify insights and potential gaps in data.\n",
    "\n",
    "- Bar Chart -> Top 20 stations highest exits, entries, traffic \n",
    "- Line chart -> Consistency of Entries and Exits over time for a station (We're looking for consistent traffic)\n",
    "- Scatter Plot -> Exits versus Entries for a particular station \n",
    "- Heatmat -> Traffic flow during the weekday by TIME of a particular station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADDING FARE AND LOCATION DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUTURE IDEAS\n",
    "\n",
    "1. Another way to estimate the number of daily regular schedule commuters at a station is to find the total number of unlimited and student metro cards used daily."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
